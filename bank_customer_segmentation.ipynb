{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#  Bank Customer Segmentation & Regional Transaction Volume Forecasting\n",
                "\n",
                "### AI in Finance | Economics & Business Analytics\n",
                "\n",
                "**Techniques Used:** K-Means Clustering · Linear Regression · Logistic Regression\n",
                "\n",
                "---\n",
                "\n",
                "This notebook includes:\n",
                "\n",
                "- Revenue & Time-Series Analysis\n",
                "- Regional & Domain-wise EDA (Exploratory Data Analysis)\n",
                "- K-Means Clustering for Market Segmentation\n",
                "- Linear Regression for Revenue Forecasting\n",
                "- Logistic Regression for High-Volume Classification\n",
                "- Business Interpretation with Strategic Recommendations\n",
                "\n",
                "---\n",
                "\n",
                "##  Business Problem Statement\n",
                "\n",
                "Banks face the challenge of understanding diverse customer bases and predicting future revenue streams across different regions. This project addresses two key questions:\n",
                "\n",
                "1. **Customer Segmentation**: Who are our customers? What behavioral patterns define distinct location clusters?\n",
                "2. **Revenue Forecasting**: How will transaction volumes grow by region in the future?\n",
                "\n",
                "By answering these questions, banks can optimize:\n",
                "- **Pricing Strategy**: Tailor fees and interest rates per segment\n",
                "- **Risk Management**: Identify high-risk segments prone to default or attrition\n",
                "- **Resource Allocation**: Direct marketing and operational budgets to high-value regions\n",
                "- **Demand-Supply Optimization**: Match banking services supply with regional demand forecasts\n",
                "\n",
                "##  Dataset\n",
                "[Massive Bank Dataset (1 Million Rows)](https://www.kaggle.com/datasets/ksabishek/massive-bank-dataset-1-million-rows)\n",
                "\n",
                "*(Using the locally cleansed `bankdataset.csv` for specific domain and transaction mapping)*"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "##  1. Install & Import Libraries\n",
                "\n",
                "> **Why these libraries?**\n",
                "> - `pandas` & `numpy`: Data manipulation and numerical computation\n",
                "> - `matplotlib` & `seaborn` & `plotly`: Visualization for EDA\n",
                "> - `sklearn`: Machine Learning models (KMeans, Linear/Logistic Regression, StandardScaler)\n",
                "> - `joblib`: Saving trained models for deployment via Streamlit"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages (uncomment in Colab)\n",
                "# !pip install scikit-learn plotly seaborn matplotlib pandas numpy joblib\n",
                "\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import plotly.express as px\n",
                "\n",
                "# Machine Learning\n",
                "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
                "from sklearn.cluster import KMeans\n",
                "from sklearn.decomposition import PCA\n",
                "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
                "from sklearn.metrics import (\n",
                "    silhouette_score, mean_squared_error, r2_score,\n",
                "    classification_report, confusion_matrix, accuracy_score\n",
                ")\n",
                "from sklearn.model_selection import train_test_split\n",
                "import joblib\n",
                "\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "pd.set_option('display.max_columns', 50)\n",
                "pd.set_option('display.float_format', '{:.2f}'.format)\n",
                "plt.rcParams['figure.figsize'] = (12, 6)\n",
                "\n",
                "print(\" All libraries imported successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "##  2. Data Loading\n",
                "\n",
                "We load the bank transaction dataset which contains **Date, Domain, Location, Value, and Transaction_count** columns.\n",
                "\n",
                "We also engineer additional time-series features (`Year`, `Quarter`, `Month`) and compute `AvgTxnValue` (average value per transaction)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = pd.read_csv('bankdataset.csv')\n",
                "df['Date'] = pd.to_datetime(df['Date'], format='mixed')\n",
                "df['Year'] = df['Date'].dt.year\n",
                "df['Quarter'] = df['Date'].dt.quarter\n",
                "df['Month'] = df['Date'].dt.month\n",
                "df['YearMonth'] = df['Date'].dt.to_period('M')\n",
                "df['AvgTxnValue'] = (df['Value'] / df['Transaction_count']).round(2)\n",
                "\n",
                "print(f\" Dataset loaded: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
                "print(f\"   Date Range: {df['Date'].min().date()} to {df['Date'].max().date()}\")\n",
                "print(f\"   Locations: {df['Location'].nunique()} | Domains: {df['Domain'].nunique()}\")\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "##  3. Data Cleaning & Preprocessing\n",
                "\n",
                "Before any analysis, we must ensure the data is clean:\n",
                "1. Check for **null/missing values**\n",
                "2. Remove **duplicate records**\n",
                "3. Verify **data types** are correct"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 60)\n",
                "print(\" DATASET OVERVIEW\")\n",
                "print(\"=\" * 60)\n",
                "print(f\"\\nShape: {df.shape}\")\n",
                "print(f\"\\nData Types:\\n{df.dtypes}\")\n",
                "print(f\"\\n Missing Values:\")\n",
                "print(df.isnull().sum())\n",
                "print(f\"\\n Descriptive Statistics:\")\n",
                "df.describe()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_clean = df.copy()\n",
                "\n",
                "# Remove exact duplicates\n",
                "before = len(df_clean)\n",
                "df_clean.drop_duplicates(inplace=True)\n",
                "print(f\" Duplicates removed: {before - len(df_clean):,}\")\n",
                "print(f\" Clean dataset shape: {df_clean.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "##  4. Exploratory Data Analysis (EDA)\n",
                "\n",
                "EDA helps us understand patterns, anomalies, and relationships in data before building models.\n",
                "\n",
                "We explore:\n",
                "1. **Monthly Revenue Trend** — Time-series view of total transaction volume (macroeconomic flow)\n",
                "2. **Top Locations by Value** — Geographic concentration of revenue\n",
                "3. **Domain Distribution** — Which business sectors dominate?\n",
                "4. **Domain Revenue Over Time** — How sectors compete quarterly\n",
                "\n",
                "###  Business Case Study Questions\n",
                "- Which location generates the highest processing volume? → *Direct capital allocation*\n",
                "- Which business domain dominates transaction counts? → *Infrastructure scaling decisions*\n",
                "- How do we optimize infrastructure capacity based on regional hubs? → *Demand-Supply planning*\n",
                "- Should the company focus on High-Value low-count regions or High-Count low-value regions? → *Pricing strategy*"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4.1 Monthly Revenue Trend (Time Series)\n",
                "monthly_sales = df_clean.groupby('YearMonth')['Value'].sum()\n",
                "plt.figure(figsize=(14, 5))\n",
                "monthly_sales.plot(kind='line', marker='o', color='#2c3e50', linewidth=2)\n",
                "plt.title('Monthly Revenue Trend (Macroeconomic Flow)', fontsize=14, fontweight='bold')\n",
                "plt.ylabel('Total Value (₹)')\n",
                "plt.xlabel('Month')\n",
                "plt.grid(alpha=0.3)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\n Business Insight: The line chart shows monthly revenue flow.\")\n",
                "print(\"   → Spikes indicate seasonal demand shifts requiring operational scaling.\")\n",
                "print(\"   → Dips may signal market slowdowns or reduced banking activity periods.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4.2 Top 10 Locations by Processing Volume\n",
                "loc_stats_eda = df_clean.groupby('Location')['Value'].sum().reset_index()\n",
                "loc_stats_eda = loc_stats_eda.sort_values('Value', ascending=False)\n",
                "\n",
                "fig = px.bar(\n",
                "    loc_stats_eda.head(10), x='Location', y='Value',\n",
                "    title='Top 10 Locations by Processing Volume (₹)',\n",
                "    color_discrete_sequence=['#2c3e50']\n",
                ")\n",
                "fig.show()\n",
                "\n",
                "print(\"\\n Business Insight: Geographic Revenue Concentration\")\n",
                "print(\"   → A few locations contribute disproportionately to total revenue.\")\n",
                "print(\"   → This is the Pareto Principle (80/20 rule) applied to banking geography.\")\n",
                "print(\"   → Strategy: Focus premium services and dedicated relationship managers in top hubs.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4.3 Domain Distribution (Transaction Count)\n",
                "dom_stats = df_clean.groupby('Domain')['Transaction_count'].sum().reset_index()\n",
                "\n",
                "fig2 = px.pie(\n",
                "    dom_stats, names='Domain', values='Transaction_count',\n",
                "    title='Transaction Count by Business Domain',\n",
                "    color_discrete_sequence=px.colors.qualitative.Set2\n",
                ")\n",
                "fig2.show()\n",
                "\n",
                "print(\"\\n Business Insight: Domain Market Share\")\n",
                "print(\"   → Domains with the highest transaction counts need the most server capacity.\")\n",
                "print(\"   → Domains with fewer but higher-value transactions may be more profitable per unit.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4.4 Domain Revenue Over Time (Stacked Bar)\n",
                "dom_time = df_clean.groupby(['Year', 'Quarter', 'Domain'])['Value'].sum().reset_index()\n",
                "dom_time['Period'] = dom_time['Year'].astype(str) + ' Q' + dom_time['Quarter'].astype(str)\n",
                "\n",
                "fig3 = px.bar(\n",
                "    dom_time, x='Period', y='Value', color='Domain',\n",
                "    barmode='stack', title='Domain Revenue Over Time (Quarterly)',\n",
                "    color_discrete_sequence=px.colors.qualitative.Set2\n",
                ")\n",
                "fig3.show()\n",
                "\n",
                "print(\"\\n Business Insight: Temporal Market Share Dynamics\")\n",
                "print(\"   → Watch for domains gaining or losing share over quarters.\")\n",
                "print(\"   → Growing domains = investment opportunity. Shrinking = risk signal.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4.5 Correlation Heatmap\n",
                "plt.figure(figsize=(8, 5))\n",
                "numeric_cols = df_clean[['Value', 'Transaction_count', 'AvgTxnValue', 'Year', 'Quarter', 'Month']]\n",
                "sns.heatmap(numeric_cols.corr(), annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
                "plt.title('Feature Correlation Matrix', fontsize=13, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\n Business Insight: Feature Relationships\")\n",
                "print(\"   → Strong correlation between Value and Transaction_count confirms volume-driven revenue.\")\n",
                "print(\"   → Weak temporal correlations suggest no strong seasonal bias in the dataset.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "##  5. K-Means Clustering — Location Market Segmentation\n",
                "\n",
                "###  Why Do We Need StandardScaler?\n",
                "\n",
                "Most ML algorithms (like KMeans, Logistic Regression) are **distance-based** or **gradient-based**.\n",
                "If features are on different scales, the algorithm becomes biased toward features with larger magnitudes.\n",
                "\n",
                "**Example:**\n",
                "- `TotalTxns` → values might be ~1,000\n",
                "- `TotalValue` → values might be ~50,000,000\n",
                "\n",
                "If we directly apply KMeans, `TotalValue` completely **dominates** the distance calculation and `TotalTxns` becomes mathematically irrelevant.\n",
                "\n",
                "**What StandardScaler Does:**\n",
                "It converts each feature into a **standard normal distribution** (Mean = 0, Std = 1) so all features contribute equally to the model.\n",
                "\n",
                "###  Clustering Goal\n",
                "We aggregate transaction data per location and cluster locations into 4 market segments:\n",
                "- **Premium Markets**: High value, lower volumes → Focus on elite services\n",
                "- **Volume Hubs**: Massive counts, lower avg values → Focus on operational efficiency\n",
                "- **Balanced Zones**: Steady combinations of value and volume\n",
                "- **Emerging Areas**: Low current volume but high growth opportunity"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Aggregate location-level features (same 5 features as app.py)\n",
                "loc_features = df_clean.groupby('Location').agg(\n",
                "    TotalValue=('Value', 'sum'),\n",
                "    TotalTxns=('Transaction_count', 'sum'),\n",
                "    AvgTxnValue=('AvgTxnValue', 'mean'),\n",
                "    DistinctDomains=('Domain', 'nunique'),\n",
                "    DaysActive=('Date', 'nunique')\n",
                ").reset_index()\n",
                "\n",
                "print(f\" Location features computed for {len(loc_features)} locations\")\n",
                "loc_features.head(10)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Scale features using StandardScaler\n",
                "feats = ['TotalValue', 'TotalTxns', 'AvgTxnValue', 'DistinctDomains', 'DaysActive']\n",
                "scaler = StandardScaler()\n",
                "X_scaled = scaler.fit_transform(loc_features[feats])\n",
                "\n",
                "# Fit KMeans with 4 clusters\n",
                "kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
                "loc_features['Cluster'] = kmeans.fit_predict(X_scaled)\n",
                "\n",
                "# Map cluster numbers to meaningful business segment names\n",
                "cluster_names = {0: 'PREMIUM MARKETS', 1: 'VOLUME HUBS', 2: 'BALANCED ZONES', 3: 'EMERGING AREAS'}\n",
                "loc_features['Segment'] = loc_features['Cluster'].map(cluster_names)\n",
                "\n",
                "print(\" K-Means Clustering Complete!\")\n",
                "print(f\"\\n Silhouette Score: {silhouette_score(X_scaled, loc_features['Cluster']):.4f}\")\n",
                "print(\"   (Ranges from -1 to 1. Higher = better separated clusters.)\")\n",
                "print(f\"\\n Segment Distribution:\")\n",
                "print(loc_features['Segment'].value_counts())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cluster Profile Summary\n",
                "cluster_profile = loc_features.groupby('Segment')[['TotalValue', 'TotalTxns', 'AvgTxnValue', 'DistinctDomains', 'DaysActive']].mean().round(2)\n",
                "print(\" Average Profile per Segment:\")\n",
                "cluster_profile"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize clusters using PCA (2D projection)\n",
                "pca = PCA(n_components=2, random_state=42)\n",
                "coords = pca.fit_transform(X_scaled)\n",
                "pca_df = pd.DataFrame(coords, columns=['PC1', 'PC2'])\n",
                "pca_df['Segment'] = loc_features['Segment']\n",
                "pca_df['Location'] = loc_features['Location']\n",
                "\n",
                "fig = px.scatter(\n",
                "    pca_df, x='PC1', y='PC2', color='Segment', text='Location',\n",
                "    title='K-Means Clusters (PCA 2D Projection)',\n",
                "    opacity=0.8, color_discrete_sequence=px.colors.qualitative.Set1\n",
                ")\n",
                "fig.update_traces(marker_size=10, textposition='top center', textfont_size=8)\n",
                "fig.show()\n",
                "\n",
                "print(\"\\n Business Insight: Cluster Visualization\")\n",
                "print(\"   → Well-separated clusters = KMeans has found genuinely distinct market segments.\")\n",
                "print(\"   → Overlapping clusters may indicate that some locations behave similarly.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "##  6. Linear Regression — Revenue Volume Forecasting\n",
                "\n",
                "### Business Question:\n",
                "*Can we predict how much revenue a specific Location + Domain combination will generate in the next quarter, given its transaction volume and time trend?*\n",
                "\n",
                "We use **Linear Regression** because:\n",
                "- Our target (`TotalValue`) is a continuous variable.\n",
                "- We want to understand the **linear relationship** between time, location, domain, and revenue.\n",
                "- The model coefficient tells us how much each unit increase in a feature changes the predicted revenue."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Build quarterly aggregated data for regression\n",
                "reg_df = df_clean.groupby(['Location', 'Domain', 'Year', 'Quarter']).agg(\n",
                "    TotalValue=('Value', 'sum'),\n",
                "    TotalTxns=('Transaction_count', 'sum')\n",
                ").reset_index()\n",
                "\n",
                "# Create a numeric time index (quarter number from the start)\n",
                "reg_df['TimeIndex'] = (reg_df['Year'] - reg_df['Year'].min()) * 4 + reg_df['Quarter']\n",
                "\n",
                "# Encode categorical variables\n",
                "le_loc = LabelEncoder()\n",
                "le_dom = LabelEncoder()\n",
                "reg_df['LocEnc'] = le_loc.fit_transform(reg_df['Location'])\n",
                "reg_df['DomEnc'] = le_dom.fit_transform(reg_df['Domain'])\n",
                "\n",
                "print(f\" Regression dataset prepared: {reg_df.shape[0]:,} rows\")\n",
                "reg_df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train-Test Split for Linear Regression\n",
                "features_lr = ['TimeIndex', 'LocEnc', 'DomEnc', 'TotalTxns']\n",
                "\n",
                "X_lr = reg_df[features_lr]\n",
                "y_lr = reg_df['TotalValue']\n",
                "\n",
                "X_train_lr, X_test_lr, y_train_lr, y_test_lr = train_test_split(\n",
                "    X_lr, y_lr, test_size=0.2, random_state=42\n",
                ")\n",
                "\n",
                "# Scale features\n",
                "lr_scaler = StandardScaler()\n",
                "X_train_lr_sc = lr_scaler.fit_transform(X_train_lr)\n",
                "X_test_lr_sc = lr_scaler.transform(X_test_lr)\n",
                "\n",
                "# Train Linear Regression\n",
                "lr = LinearRegression()\n",
                "lr.fit(X_train_lr_sc, y_train_lr)\n",
                "\n",
                "# Predictions\n",
                "y_pred_lr = lr.predict(X_test_lr_sc)\n",
                "\n",
                "# Evaluation\n",
                "r2 = r2_score(y_test_lr, y_pred_lr)\n",
                "rmse = np.sqrt(mean_squared_error(y_test_lr, y_pred_lr))\n",
                "\n",
                "print(\"=\" * 50)\n",
                "print(\" LINEAR REGRESSION RESULTS\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"R² Score (Test): {r2:.4f}\")\n",
                "print(f\"RMSE:           ₹{rmse:,.2f}\")\n",
                "print(f\"\\n→ R² of {r2:.4f} means the model explains {r2*100:.1f}% of revenue variance.\")\n",
                "print(f\"→ RMSE of ₹{rmse:,.0f} is the average prediction error per observation.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize: Actual vs Predicted\n",
                "plt.figure(figsize=(10, 5))\n",
                "plt.scatter(y_test_lr, y_pred_lr, alpha=0.4, color='#2c3e50', s=15)\n",
                "plt.plot([y_test_lr.min(), y_test_lr.max()], [y_test_lr.min(), y_test_lr.max()], 'r--', lw=2, label='Perfect Prediction')\n",
                "plt.xlabel('Actual Value (₹)')\n",
                "plt.ylabel('Predicted Value (₹)')\n",
                "plt.title('Linear Regression: Actual vs Predicted Revenue', fontsize=13, fontweight='bold')\n",
                "plt.legend()\n",
                "plt.grid(alpha=0.3)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\n Business Insight:\")\n",
                "print(\"   → Points close to the red line = accurate predictions.\")\n",
                "print(\"   → Spread from the line = variance the model cannot yet capture.\")\n",
                "print(\"   → This model is used in the deployed Streamlit app for the Predictor feature.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "##  7. Logistic Regression — High-Volume Hub Classification\n",
                "\n",
                "### Business Question:\n",
                "*Can we classify whether a Location–Domain combination will be \"High Volume\" (above median revenue) or \"Low Volume\"?*\n",
                "\n",
                "This is a **binary classification** problem. Instead of predicting an exact number (regression), we predict a **category** (High or Low).\n",
                "\n",
                "**Why this matters:**\n",
                "- Banks can proactively allocate server capacity and staffing to predicted high-volume hubs.\n",
                "- Missing a high-volume hub (Type II error / False Negative) is expensive — the bank loses revenue from inadequate infrastructure."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Create Target Variable\n",
                "# High_Volume = 1 if TotalValue > median, else 0\n",
                "median_val = reg_df['TotalValue'].median()\n",
                "reg_df['High_Volume'] = (reg_df['TotalValue'] > median_val).astype(int)\n",
                "\n",
                "print(f\" Revenue Median Threshold: ₹{median_val:,.0f}\")\n",
                "print(f\"   Above median (High Volume = 1): {reg_df['High_Volume'].sum():,} records\")\n",
                "print(f\"   Below median (Low Volume  = 0): {(reg_df['High_Volume'] == 0).sum():,} records\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Train-Test Split\n",
                "features_cls = ['TimeIndex', 'LocEnc', 'DomEnc', 'TotalTxns']\n",
                "X_cls = reg_df[features_cls]\n",
                "y_cls = reg_df['High_Volume']\n",
                "\n",
                "X_train_cls, X_test_cls, y_train_cls, y_test_cls = train_test_split(\n",
                "    X_cls, y_cls, test_size=0.3, random_state=42\n",
                ")\n",
                "\n",
                "# 3. Scale Features\n",
                "scaler_cls = StandardScaler()\n",
                "X_train_cls_sc = scaler_cls.fit_transform(X_train_cls)\n",
                "X_test_cls_sc = scaler_cls.transform(X_test_cls)\n",
                "\n",
                "# 4. Train Logistic Regression\n",
                "log_model = LogisticRegression(random_state=42)\n",
                "log_model.fit(X_train_cls_sc, y_train_cls)\n",
                "\n",
                "# 5. Predict\n",
                "y_pred_cls = log_model.predict(X_test_cls_sc)\n",
                "\n",
                "# 6. Evaluation\n",
                "acc = accuracy_score(y_test_cls, y_pred_cls)\n",
                "cm = confusion_matrix(y_test_cls, y_pred_cls)\n",
                "cr = classification_report(y_test_cls, y_pred_cls)\n",
                "\n",
                "print(\"=\" * 50)\n",
                "print(\" LOGISTIC REGRESSION RESULTS\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"\\nAccuracy: {acc:.4f} ({acc*100:.1f}%)\")\n",
                "print(f\"\\nConfusion Matrix:\")\n",
                "print(cm)\n",
                "print(f\"\\nClassification Report:\")\n",
                "print(cr)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "###  Understanding the Confusion Matrix\n",
                "\n",
                "```\n",
                "                    Predicted Low    Predicted High\n",
                "Actual Low (0)      TN               FP (Type I)\n",
                "Actual High (1)     FN (Type II)     TP\n",
                "```\n",
                "\n",
                "| Metric | Meaning |\n",
                "|--------|---------|\n",
                "| **Precision** | When the model says \"High Volume\", how often is it correct? |\n",
                "| **Recall** | Out of all actual High Volume hubs, what % does the model find? |\n",
                "| **F1-Score** | Harmonic mean of Precision and Recall (balanced metric) |\n",
                "| **Type I Error (FP)** | Model says High, but actually Low → Wasted resources |\n",
                "| **Type II Error (FN)** | Model says Low, but actually High → **Costly miss!** Lost revenue |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Confusion Matrix Heatmap\n",
                "plt.figure(figsize=(6, 4))\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
                "            xticklabels=['Predicted Low', 'Predicted High'],\n",
                "            yticklabels=['Actual Low', 'Actual High'])\n",
                "plt.title('Confusion Matrix: High Volume Classification', fontsize=12, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\n Business Insight: Classification Performance\")\n",
                "print(\"   → High recall for class 1 = the model catches most high-volume hubs (good!).\")\n",
                "print(\"   → Low recall = the model misses real high-volume locations (expensive mistake).\")\n",
                "print(\"   → This classification helps the bank preemptively scale infrastructure.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "##  8. Business Interpretation of Results\n",
                "\n",
                "### 8.1 Segment Strategy\n",
                "\n",
                "| Segment | Characteristics | Strategy |\n",
                "|---------|----------------|----------|\n",
                "| **Premium Markets** | High value, lower transaction volume | Upsell premium services, assign dedicated relationship managers, white-glove support |\n",
                "| **Volume Hubs** | Massive transaction counts, lower average values | Optimize server and infrastructure capacity; reduce per-transaction costs through automation |\n",
                "| **Balanced Zones** | Steady combination of value and volume | Maintain current service levels; cross-sell across domains |\n",
                "| **Emerging Areas** | Low current volume, high growth potential | Target localized marketing campaigns; capture emerging market share before competitors |\n",
                "\n",
                "### 8.2 Economic Concepts Applied\n",
                "\n",
                "| Concept | Application in This Project |\n",
                "|---------|----------------------------|\n",
                "| **Demand-Supply** | K-Means identifies demand concentration by location; supply (banking infra) should match |\n",
                "| **Revenue Optimization** | Linear Regression forecasts revenue; helps plan quarterly targets |\n",
                "| **Pricing Strategy** | Premium Markets can sustain higher fees; Volume Hubs need competitive low-cost processing |\n",
                "| **Risk Analysis** | Emerging Areas are high-risk investments; Volume Hubs face operational risk from overload |\n",
                "| **Market Segmentation** | K-Means provides data-driven segments instead of subjective categories |\n",
                "\n",
                "### 8.3 Strategic Recommendations\n",
                "\n",
                "| Finding | Business Strategy Recommendation |\n",
                "|---------|----------------------------------|\n",
                "| Premium Markets drive disproportionate value | Upsell premium business services to these regions, provide white-glove support. |\n",
                "| Volume Hubs dominate transaction counts | Optimize server and technical infrastructure to handle peak load continuously. |\n",
                "| Emerging Areas show strong temporal growth | Target localized marketing campaigns to capture emerging market territory early. |\n",
                "| Domain concentration varies quarterly | Diversify service offerings to reduce dependency on any single domain. |\n",
                "| Revenue is predictable via regression | Use the Linear Regression model to set quarterly revenue targets per region. |\n",
                "\n",
                "### 8.4 Case Study Answers\n",
                "\n",
                "1. **Which location generates highest processing volume?** → See EDA Section 4.2. The top location drives the most ₹ through the system.\n",
                "2. **Which domain dominates transaction counts?** → See EDA Section 4.3. The largest pie slice indicates the dominant domain.\n",
                "3. **How to optimize infrastructure?** → Use K-Means segments: scale infra in Volume Hubs, premium services in Premium Markets.\n",
                "4. **High-Value vs High-Count strategy?** → Both matter. Use segment-specific strategies as outlined in Section 8.1."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "##  9. Save Models for Deployment\n",
                "\n",
                "These pickled models are loaded by the **Streamlit** dashboard (`app.py`) for live predictions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save models for Streamlit deployment\n",
                "joblib.dump(kmeans, 'kmeans_model.pkl')\n",
                "joblib.dump(scaler, 'scaler.pkl')\n",
                "joblib.dump(lr, 'lr_model.pkl')\n",
                "\n",
                "print(\" Models saved successfully!\")\n",
                "print(\"   • kmeans_model.pkl  → K-Means (4-cluster segmentation)\")\n",
                "print(\"   • scaler.pkl        → StandardScaler (feature normalization)\")\n",
                "print(\"   • lr_model.pkl      → LinearRegression (revenue forecasting)\")\n",
                "print(\"\\n These are used by the Streamlit app (app.py) for the interactive dashboard and predictor interface.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "##  Deployment\n",
                "\n",
                "The project is deployed using **Streamlit** and includes:\n",
                "\n",
                "1. **Interactive Dashboard** — Overview, Market Segments, Domains, Forecast pages\n",
                "2. **Prediction Interface** — Select Location + Domain + Transaction Volume to get a Revenue forecast\n",
                "3. **Clear Input-Output Demo** — Users can experiment with different combinations and see real-time results\n",
                "\n",
                "### To Run Locally:\n",
                "```bash\n",
                "pip install -r requirements.txt\n",
                "streamlit run app.py\n",
                "```\n",
                "\n",
                "### GitHub Repository\n",
                "The repository contains:\n",
                "- `bank_customer_segmentation.ipynb` — This notebook (analysis + models)\n",
                "- `app.py` — Streamlit application\n",
                "- `bankdataset.csv` — Dataset\n",
                "- `requirements.txt` — Dependencies\n",
                "- `README.md` — Detailed documentation"
            ]
        }
    ],
    "metadata": {
        "colab": {
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.13.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
