{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# \ud83c\udfe6 Bank Customer Segmentation & Regional Transaction Volume Forecasting\n",
                "\n",
                "### AI in Finance | Economics & Business Analytics\n",
                "\n",
                "**Techniques Used:** K-Means Clustering \u00b7 Linear Regression \u00b7 Logistic Regression\n",
                "\n",
                "---\n",
                "\n",
                "This notebook includes:\n",
                "\n",
                "- Revenue & Time-Series Analysis\n",
                "- Regional & Domain-wise EDA (Exploratory Data Analysis)\n",
                "- K-Means Clustering for Market Segmentation\n",
                "- Linear Regression for Revenue Forecasting\n",
                "- Logistic Regression for High-Volume Classification\n",
                "- Business Interpretation with Strategic Recommendations\n",
                "\n",
                "---\n",
                "\n",
                "## \ud83d\udccc Business Problem Statement\n",
                "\n",
                "Banks face the challenge of understanding diverse customer bases and predicting future revenue streams across different regions. This project addresses two key questions:\n",
                "\n",
                "1. **Customer Segmentation**: Who are our customers? What behavioral patterns define distinct location clusters?\n",
                "2. **Revenue Forecasting**: How will transaction volumes grow by region in the future?\n",
                "\n",
                "By answering these questions, banks can optimize:\n",
                "- **Pricing Strategy**: Tailor fees and interest rates per segment\n",
                "- **Risk Management**: Identify high-risk segments prone to default or attrition\n",
                "- **Resource Allocation**: Direct marketing and operational budgets to high-value regions\n",
                "- **Demand-Supply Optimization**: Match banking services supply with regional demand forecasts\n",
                "\n",
                "## \ud83d\udd17 Dataset\n",
                "[Massive Bank Dataset (1 Million Rows)](https://www.kaggle.com/datasets/ksabishek/massive-bank-dataset-1-million-rows)\n",
                "\n",
                "*(Using the locally cleansed `bankdataset.csv` for specific domain and transaction mapping)*"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## \ud83d\udce6 1. Install & Import Libraries\n",
                "\n",
                "> **Why these libraries?**\n",
                "> - `pandas` & `numpy`: Data manipulation and numerical computation\n",
                "> - `matplotlib` & `seaborn` & `plotly`: Visualization for EDA\n",
                "> - `sklearn`: Machine Learning models (KMeans, Linear/Logistic Regression, StandardScaler)\n",
                "> - `joblib`: Saving trained models for deployment via Streamlit"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages (uncomment in Colab)\n",
                "# !pip install scikit-learn plotly seaborn matplotlib pandas numpy joblib\n",
                "\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import plotly.express as px\n",
                "\n",
                "# Machine Learning\n",
                "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
                "from sklearn.cluster import KMeans\n",
                "from sklearn.decomposition import PCA\n",
                "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
                "from sklearn.metrics import (\n",
                "    silhouette_score, mean_squared_error, r2_score,\n",
                "    classification_report, confusion_matrix, accuracy_score\n",
                ")\n",
                "from sklearn.model_selection import train_test_split\n",
                "import joblib\n",
                "\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "pd.set_option('display.max_columns', 50)\n",
                "pd.set_option('display.float_format', '{:.2f}'.format)\n",
                "plt.rcParams['figure.figsize'] = (12, 6)\n",
                "\n",
                "print(\"\u2705 All libraries imported successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## \ud83d\udce5 2. Data Loading\n",
                "\n",
                "We load the bank transaction dataset which contains **Date, Domain, Location, Value, and Transaction_count** columns.\n",
                "\n",
                "We also engineer additional time-series features (`Year`, `Quarter`, `Month`) and compute `AvgTxnValue` (average value per transaction)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = pd.read_csv('bankdataset.csv')\n",
                "df['Date'] = pd.to_datetime(df['Date'], format='mixed')\n",
                "df['Year'] = df['Date'].dt.year\n",
                "df['Quarter'] = df['Date'].dt.quarter\n",
                "df['Month'] = df['Date'].dt.month\n",
                "df['YearMonth'] = df['Date'].dt.to_period('M')\n",
                "df['AvgTxnValue'] = (df['Value'] / df['Transaction_count']).round(2)\n",
                "\n",
                "print(f\"\u2705 Dataset loaded: {df.shape[0]:,} rows \u00d7 {df.shape[1]} columns\")\n",
                "print(f\"   Date Range: {df['Date'].min().date()} to {df['Date'].max().date()}\")\n",
                "print(f\"   Locations: {df['Location'].nunique()} | Domains: {df['Domain'].nunique()}\")\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## \ud83e\uddf9 3. Data Cleaning & Preprocessing\n",
                "\n",
                "Before any analysis, we must ensure the data is clean:\n",
                "1. Check for **null/missing values**\n",
                "2. Remove **duplicate records**\n",
                "3. Verify **data types** are correct"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 60)\n",
                "print(\"\ud83d\udcca DATASET OVERVIEW\")\n",
                "print(\"=\" * 60)\n",
                "print(f\"\\nShape: {df.shape}\")\n",
                "print(f\"\\nData Types:\\n{df.dtypes}\")\n",
                "print(f\"\\n\ud83d\udccc Missing Values:\")\n",
                "print(df.isnull().sum())\n",
                "print(f\"\\n\ud83d\udccc Descriptive Statistics:\")\n",
                "df.describe()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_clean = df.copy()\n",
                "\n",
                "# Remove exact duplicates\n",
                "before = len(df_clean)\n",
                "df_clean.drop_duplicates(inplace=True)\n",
                "print(f\"\ud83d\udd01 Duplicates removed: {before - len(df_clean):,}\")\n",
                "print(f\"\u2705 Clean dataset shape: {df_clean.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## \ud83d\udcca 4. Exploratory Data Analysis (EDA)\n",
                "\n",
                "EDA helps us understand patterns, anomalies, and relationships in data before building models.\n",
                "\n",
                "We explore:\n",
                "1. **Monthly Revenue Trend** \u2014 Time-series view of total transaction volume (macroeconomic flow)\n",
                "2. **Top Locations by Value** \u2014 Geographic concentration of revenue\n",
                "3. **Domain Distribution** \u2014 Which business sectors dominate?\n",
                "4. **Domain Revenue Over Time** \u2014 How sectors compete quarterly\n",
                "\n",
                "### \ud83d\udcac Business Case Study Questions\n",
                "- Which location generates the highest processing volume? \u2192 *Direct capital allocation*\n",
                "- Which business domain dominates transaction counts? \u2192 *Infrastructure scaling decisions*\n",
                "- How do we optimize infrastructure capacity based on regional hubs? \u2192 *Demand-Supply planning*\n",
                "- Should the company focus on High-Value low-count regions or High-Count low-value regions? \u2192 *Pricing strategy*"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4.1 Monthly Revenue Trend (Time Series)\n",
                "monthly_sales = df_clean.groupby('YearMonth')['Value'].sum()\n",
                "plt.figure(figsize=(14, 5))\n",
                "monthly_sales.plot(kind='line', marker='o', color='#2c3e50', linewidth=2)\n",
                "plt.title('Monthly Revenue Trend (Macroeconomic Flow)', fontsize=14, fontweight='bold')\n",
                "plt.ylabel('Total Value (\u20b9)')\n",
                "plt.xlabel('Month')\n",
                "plt.grid(alpha=0.3)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\n\ud83d\udcca Business Insight: The line chart shows monthly revenue flow.\")\n",
                "print(\"   \u2192 Spikes indicate seasonal demand shifts requiring operational scaling.\")\n",
                "print(\"   \u2192 Dips may signal market slowdowns or reduced banking activity periods.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4.2 Top 10 Locations by Processing Volume\n",
                "loc_stats_eda = df_clean.groupby('Location')['Value'].sum().reset_index()\n",
                "loc_stats_eda = loc_stats_eda.sort_values('Value', ascending=False)\n",
                "\n",
                "fig = px.bar(\n",
                "    loc_stats_eda.head(10), x='Location', y='Value',\n",
                "    title='Top 10 Locations by Processing Volume (\u20b9)',\n",
                "    color_discrete_sequence=['#2c3e50']\n",
                ")\n",
                "fig.show()\n",
                "\n",
                "print(\"\\n\ud83d\udcca Business Insight: Geographic Revenue Concentration\")\n",
                "print(\"   \u2192 A few locations contribute disproportionately to total revenue.\")\n",
                "print(\"   \u2192 This is the Pareto Principle (80/20 rule) applied to banking geography.\")\n",
                "print(\"   \u2192 Strategy: Focus premium services and dedicated relationship managers in top hubs.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4.3 Domain Distribution (Transaction Count)\n",
                "dom_stats = df_clean.groupby('Domain')['Transaction_count'].sum().reset_index()\n",
                "\n",
                "fig2 = px.pie(\n",
                "    dom_stats, names='Domain', values='Transaction_count',\n",
                "    title='Transaction Count by Business Domain',\n",
                "    color_discrete_sequence=px.colors.qualitative.Set2\n",
                ")\n",
                "fig2.show()\n",
                "\n",
                "print(\"\\n\ud83d\udcca Business Insight: Domain Market Share\")\n",
                "print(\"   \u2192 Domains with the highest transaction counts need the most server capacity.\")\n",
                "print(\"   \u2192 Domains with fewer but higher-value transactions may be more profitable per unit.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4.4 Domain Revenue Over Time (Stacked Bar)\n",
                "dom_time = df_clean.groupby(['Year', 'Quarter', 'Domain'])['Value'].sum().reset_index()\n",
                "dom_time['Period'] = dom_time['Year'].astype(str) + ' Q' + dom_time['Quarter'].astype(str)\n",
                "\n",
                "fig3 = px.bar(\n",
                "    dom_time, x='Period', y='Value', color='Domain',\n",
                "    barmode='stack', title='Domain Revenue Over Time (Quarterly)',\n",
                "    color_discrete_sequence=px.colors.qualitative.Set2\n",
                ")\n",
                "fig3.show()\n",
                "\n",
                "print(\"\\n\ud83d\udcca Business Insight: Temporal Market Share Dynamics\")\n",
                "print(\"   \u2192 Watch for domains gaining or losing share over quarters.\")\n",
                "print(\"   \u2192 Growing domains = investment opportunity. Shrinking = risk signal.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4.5 Correlation Heatmap\n",
                "plt.figure(figsize=(8, 5))\n",
                "numeric_cols = df_clean[['Value', 'Transaction_count', 'AvgTxnValue', 'Year', 'Quarter', 'Month']]\n",
                "sns.heatmap(numeric_cols.corr(), annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
                "plt.title('Feature Correlation Matrix', fontsize=13, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\n\ud83d\udcca Business Insight: Feature Relationships\")\n",
                "print(\"   \u2192 Strong correlation between Value and Transaction_count confirms volume-driven revenue.\")\n",
                "print(\"   \u2192 Weak temporal correlations suggest no strong seasonal bias in the dataset.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## \ud83e\udd16 5. K-Means Clustering \u2014 Location Market Segmentation\n",
                "\n",
                "### \ud83d\udca1 Why Do We Need StandardScaler?\n",
                "\n",
                "Most ML algorithms (like KMeans, Logistic Regression) are **distance-based** or **gradient-based**.\n",
                "If features are on different scales, the algorithm becomes biased toward features with larger magnitudes.\n",
                "\n",
                "**Example:**\n",
                "- `TotalTxns` \u2192 values might be ~1,000\n",
                "- `TotalValue` \u2192 values might be ~50,000,000\n",
                "\n",
                "If we directly apply KMeans, `TotalValue` completely **dominates** the distance calculation and `TotalTxns` becomes mathematically irrelevant.\n",
                "\n",
                "**What StandardScaler Does:**\n",
                "It converts each feature into a **standard normal distribution** (Mean = 0, Std = 1) so all features contribute equally to the model.\n",
                "\n",
                "### \ud83c\udfaf Clustering Goal\n",
                "We aggregate transaction data per location and cluster locations into 4 market segments:\n",
                "- **Premium Markets**: High value, lower volumes \u2192 Focus on elite services\n",
                "- **Volume Hubs**: Massive counts, lower avg values \u2192 Focus on operational efficiency\n",
                "- **Balanced Zones**: Steady combinations of value and volume\n",
                "- **Emerging Areas**: Low current volume but high growth opportunity"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Aggregate location-level features (same 5 features as app.py)\n",
                "loc_features = df_clean.groupby('Location').agg(\n",
                "    TotalValue=('Value', 'sum'),\n",
                "    TotalTxns=('Transaction_count', 'sum'),\n",
                "    AvgTxnValue=('AvgTxnValue', 'mean'),\n",
                "    DistinctDomains=('Domain', 'nunique'),\n",
                "    DaysActive=('Date', 'nunique')\n",
                ").reset_index()\n",
                "\n",
                "print(f\"\u2705 Location features computed for {len(loc_features)} locations\")\n",
                "loc_features.head(10)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Scale features using StandardScaler\n",
                "feats = ['TotalValue', 'TotalTxns', 'AvgTxnValue', 'DistinctDomains', 'DaysActive']\n",
                "scaler = StandardScaler()\n",
                "X_scaled = scaler.fit_transform(loc_features[feats])\n",
                "\n",
                "# Fit KMeans with 4 clusters\n",
                "kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
                "loc_features['Cluster'] = kmeans.fit_predict(X_scaled)\n",
                "\n",
                "# Map cluster numbers to meaningful business segment names\n",
                "cluster_names = {0: 'PREMIUM MARKETS', 1: 'VOLUME HUBS', 2: 'BALANCED ZONES', 3: 'EMERGING AREAS'}\n",
                "loc_features['Segment'] = loc_features['Cluster'].map(cluster_names)\n",
                "\n",
                "print(\"\u2705 K-Means Clustering Complete!\")\n",
                "print(f\"\\n\ud83d\udcca Silhouette Score: {silhouette_score(X_scaled, loc_features['Cluster']):.4f}\")\n",
                "print(\"   (Ranges from -1 to 1. Higher = better separated clusters.)\")\n",
                "print(f\"\\n\ud83d\udcca Segment Distribution:\")\n",
                "print(loc_features['Segment'].value_counts())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cluster Profile Summary\n",
                "cluster_profile = loc_features.groupby('Segment')[['TotalValue', 'TotalTxns', 'AvgTxnValue', 'DistinctDomains', 'DaysActive']].mean().round(2)\n",
                "print(\"\ud83d\udcca Average Profile per Segment:\")\n",
                "cluster_profile"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize clusters using PCA (2D projection)\n",
                "pca = PCA(n_components=2, random_state=42)\n",
                "coords = pca.fit_transform(X_scaled)\n",
                "pca_df = pd.DataFrame(coords, columns=['PC1', 'PC2'])\n",
                "pca_df['Segment'] = loc_features['Segment']\n",
                "pca_df['Location'] = loc_features['Location']\n",
                "\n",
                "fig = px.scatter(\n",
                "    pca_df, x='PC1', y='PC2', color='Segment', text='Location',\n",
                "    title='K-Means Clusters (PCA 2D Projection)',\n",
                "    opacity=0.8, color_discrete_sequence=px.colors.qualitative.Set1\n",
                ")\n",
                "fig.update_traces(marker_size=10, textposition='top center', textfont_size=8)\n",
                "fig.show()\n",
                "\n",
                "print(\"\\n\ud83d\udcca Business Insight: Cluster Visualization\")\n",
                "print(\"   \u2192 Well-separated clusters = KMeans has found genuinely distinct market segments.\")\n",
                "print(\"   \u2192 Overlapping clusters may indicate that some locations behave similarly.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## \ud83d\udcc9 6. Linear Regression \u2014 Revenue Volume Forecasting\n",
                "\n",
                "### Business Question:\n",
                "*Can we predict how much revenue a specific Location + Domain combination will generate in the next quarter, given its transaction volume and time trend?*\n",
                "\n",
                "We use **Linear Regression** because:\n",
                "- Our target (`TotalValue`) is a continuous variable.\n",
                "- We want to understand the **linear relationship** between time, location, domain, and revenue.\n",
                "- The model coefficient tells us how much each unit increase in a feature changes the predicted revenue."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Build quarterly aggregated data for regression\n",
                "reg_df = df_clean.groupby(['Location', 'Domain', 'Year', 'Quarter']).agg(\n",
                "    TotalValue=('Value', 'sum'),\n",
                "    TotalTxns=('Transaction_count', 'sum')\n",
                ").reset_index()\n",
                "\n",
                "# Create a numeric time index (quarter number from the start)\n",
                "reg_df['TimeIndex'] = (reg_df['Year'] - reg_df['Year'].min()) * 4 + reg_df['Quarter']\n",
                "\n",
                "# Encode categorical variables\n",
                "le_loc = LabelEncoder()\n",
                "le_dom = LabelEncoder()\n",
                "reg_df['LocEnc'] = le_loc.fit_transform(reg_df['Location'])\n",
                "reg_df['DomEnc'] = le_dom.fit_transform(reg_df['Domain'])\n",
                "\n",
                "print(f\"\u2705 Regression dataset prepared: {reg_df.shape[0]:,} rows\")\n",
                "reg_df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train-Test Split for Linear Regression\n",
                "features_lr = ['TimeIndex', 'LocEnc', 'DomEnc', 'TotalTxns']\n",
                "\n",
                "X_lr = reg_df[features_lr]\n",
                "y_lr = reg_df['TotalValue']\n",
                "\n",
                "X_train_lr, X_test_lr, y_train_lr, y_test_lr = train_test_split(\n",
                "    X_lr, y_lr, test_size=0.2, random_state=42\n",
                ")\n",
                "\n",
                "# Scale features\n",
                "lr_scaler = StandardScaler()\n",
                "X_train_lr_sc = lr_scaler.fit_transform(X_train_lr)\n",
                "X_test_lr_sc = lr_scaler.transform(X_test_lr)\n",
                "\n",
                "# Train Linear Regression\n",
                "lr = LinearRegression()\n",
                "lr.fit(X_train_lr_sc, y_train_lr)\n",
                "\n",
                "# Predictions\n",
                "y_pred_lr = lr.predict(X_test_lr_sc)\n",
                "\n",
                "# Evaluation\n",
                "r2 = r2_score(y_test_lr, y_pred_lr)\n",
                "rmse = np.sqrt(mean_squared_error(y_test_lr, y_pred_lr))\n",
                "\n",
                "print(\"=\" * 50)\n",
                "print(\"\ud83d\udcca LINEAR REGRESSION RESULTS\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"R\u00b2 Score (Test): {r2:.4f}\")\n",
                "print(f\"RMSE:           \u20b9{rmse:,.2f}\")\n",
                "print(f\"\\n\u2192 R\u00b2 of {r2:.4f} means the model explains {r2*100:.1f}% of revenue variance.\")\n",
                "print(f\"\u2192 RMSE of \u20b9{rmse:,.0f} is the average prediction error per observation.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize: Actual vs Predicted\n",
                "plt.figure(figsize=(10, 5))\n",
                "plt.scatter(y_test_lr, y_pred_lr, alpha=0.4, color='#2c3e50', s=15)\n",
                "plt.plot([y_test_lr.min(), y_test_lr.max()], [y_test_lr.min(), y_test_lr.max()], 'r--', lw=2, label='Perfect Prediction')\n",
                "plt.xlabel('Actual Value (\u20b9)')\n",
                "plt.ylabel('Predicted Value (\u20b9)')\n",
                "plt.title('Linear Regression: Actual vs Predicted Revenue', fontsize=13, fontweight='bold')\n",
                "plt.legend()\n",
                "plt.grid(alpha=0.3)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\n\ud83d\udcca Business Insight:\")\n",
                "print(\"   \u2192 Points close to the red line = accurate predictions.\")\n",
                "print(\"   \u2192 Spread from the line = variance the model cannot yet capture.\")\n",
                "print(\"   \u2192 This model is used in the deployed Streamlit app for the Predictor feature.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## \ud83d\udd0d 7. Logistic Regression \u2014 High-Volume Hub Classification\n",
                "\n",
                "### Business Question:\n",
                "*Can we classify whether a Location\u2013Domain combination will be \"High Volume\" (above median revenue) or \"Low Volume\"?*\n",
                "\n",
                "This is a **binary classification** problem. Instead of predicting an exact number (regression), we predict a **category** (High or Low).\n",
                "\n",
                "**Why this matters:**\n",
                "- Banks can proactively allocate server capacity and staffing to predicted high-volume hubs.\n",
                "- Missing a high-volume hub (Type II error / False Negative) is expensive \u2014 the bank loses revenue from inadequate infrastructure."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Create Target Variable\n",
                "# High_Volume = 1 if TotalValue > median, else 0\n",
                "median_val = reg_df['TotalValue'].median()\n",
                "reg_df['High_Volume'] = (reg_df['TotalValue'] > median_val).astype(int)\n",
                "\n",
                "print(f\"\ud83d\udcca Revenue Median Threshold: \u20b9{median_val:,.0f}\")\n",
                "print(f\"   Above median (High Volume = 1): {reg_df['High_Volume'].sum():,} records\")\n",
                "print(f\"   Below median (Low Volume  = 0): {(reg_df['High_Volume'] == 0).sum():,} records\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Train-Test Split\n",
                "features_cls = ['TimeIndex', 'LocEnc', 'DomEnc', 'TotalTxns']\n",
                "X_cls = reg_df[features_cls]\n",
                "y_cls = reg_df['High_Volume']\n",
                "\n",
                "X_train_cls, X_test_cls, y_train_cls, y_test_cls = train_test_split(\n",
                "    X_cls, y_cls, test_size=0.3, random_state=42\n",
                ")\n",
                "\n",
                "# 3. Scale Features\n",
                "scaler_cls = StandardScaler()\n",
                "X_train_cls_sc = scaler_cls.fit_transform(X_train_cls)\n",
                "X_test_cls_sc = scaler_cls.transform(X_test_cls)\n",
                "\n",
                "# 4. Train Logistic Regression\n",
                "log_model = LogisticRegression(random_state=42)\n",
                "log_model.fit(X_train_cls_sc, y_train_cls)\n",
                "\n",
                "# 5. Predict\n",
                "y_pred_cls = log_model.predict(X_test_cls_sc)\n",
                "\n",
                "# 6. Evaluation\n",
                "acc = accuracy_score(y_test_cls, y_pred_cls)\n",
                "cm = confusion_matrix(y_test_cls, y_pred_cls)\n",
                "cr = classification_report(y_test_cls, y_pred_cls)\n",
                "\n",
                "print(\"=\" * 50)\n",
                "print(\"\ud83d\udcca LOGISTIC REGRESSION RESULTS\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"\\nAccuracy: {acc:.4f} ({acc*100:.1f}%)\")\n",
                "print(f\"\\nConfusion Matrix:\")\n",
                "print(cm)\n",
                "print(f\"\\nClassification Report:\")\n",
                "print(cr)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### \ud83e\udde0 Understanding the Confusion Matrix\n",
                "\n",
                "```\n",
                "                    Predicted Low    Predicted High\n",
                "Actual Low (0)      TN               FP (Type I)\n",
                "Actual High (1)     FN (Type II)     TP\n",
                "```\n",
                "\n",
                "| Metric | Meaning |\n",
                "|--------|---------|\n",
                "| **Precision** | When the model says \"High Volume\", how often is it correct? |\n",
                "| **Recall** | Out of all actual High Volume hubs, what % does the model find? |\n",
                "| **F1-Score** | Harmonic mean of Precision and Recall (balanced metric) |\n",
                "| **Type I Error (FP)** | Model says High, but actually Low \u2192 Wasted resources |\n",
                "| **Type II Error (FN)** | Model says Low, but actually High \u2192 **Costly miss!** Lost revenue |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Confusion Matrix Heatmap\n",
                "plt.figure(figsize=(6, 4))\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
                "            xticklabels=['Predicted Low', 'Predicted High'],\n",
                "            yticklabels=['Actual Low', 'Actual High'])\n",
                "plt.title('Confusion Matrix: High Volume Classification', fontsize=12, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\n\ud83d\udcca Business Insight: Classification Performance\")\n",
                "print(\"   \u2192 High recall for class 1 = the model catches most high-volume hubs (good!).\")\n",
                "print(\"   \u2192 Low recall = the model misses real high-volume locations (expensive mistake).\")\n",
                "print(\"   \u2192 This classification helps the bank preemptively scale infrastructure.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## \ud83d\udcbc 8. Business Interpretation of Results\n",
                "\n",
                "### 8.1 Segment Strategy\n",
                "\n",
                "| Segment | Characteristics | Strategy |\n",
                "|---------|----------------|----------|\n",
                "| **Premium Markets** | High value, lower transaction volume | Upsell premium services, assign dedicated relationship managers, white-glove support |\n",
                "| **Volume Hubs** | Massive transaction counts, lower average values | Optimize server and infrastructure capacity; reduce per-transaction costs through automation |\n",
                "| **Balanced Zones** | Steady combination of value and volume | Maintain current service levels; cross-sell across domains |\n",
                "| **Emerging Areas** | Low current volume, high growth potential | Target localized marketing campaigns; capture emerging market share before competitors |\n",
                "\n",
                "### 8.2 Economic Concepts Applied\n",
                "\n",
                "| Concept | Application in This Project |\n",
                "|---------|----------------------------|\n",
                "| **Demand-Supply** | K-Means identifies demand concentration by location; supply (banking infra) should match |\n",
                "| **Revenue Optimization** | Linear Regression forecasts revenue; helps plan quarterly targets |\n",
                "| **Pricing Strategy** | Premium Markets can sustain higher fees; Volume Hubs need competitive low-cost processing |\n",
                "| **Risk Analysis** | Emerging Areas are high-risk investments; Volume Hubs face operational risk from overload |\n",
                "| **Market Segmentation** | K-Means provides data-driven segments instead of subjective categories |\n",
                "\n",
                "### 8.3 Strategic Recommendations\n",
                "\n",
                "| Finding | Business Strategy Recommendation |\n",
                "|---------|----------------------------------|\n",
                "| Premium Markets drive disproportionate value | Upsell premium business services to these regions, provide white-glove support. |\n",
                "| Volume Hubs dominate transaction counts | Optimize server and technical infrastructure to handle peak load continuously. |\n",
                "| Emerging Areas show strong temporal growth | Target localized marketing campaigns to capture emerging market territory early. |\n",
                "| Domain concentration varies quarterly | Diversify service offerings to reduce dependency on any single domain. |\n",
                "| Revenue is predictable via regression | Use the Linear Regression model to set quarterly revenue targets per region. |\n",
                "\n",
                "### 8.4 Case Study Answers\n",
                "\n",
                "1. **Which location generates highest processing volume?** \u2192 See EDA Section 4.2. The top location drives the most \u20b9 through the system.\n",
                "2. **Which domain dominates transaction counts?** \u2192 See EDA Section 4.3. The largest pie slice indicates the dominant domain.\n",
                "3. **How to optimize infrastructure?** \u2192 Use K-Means segments: scale infra in Volume Hubs, premium services in Premium Markets.\n",
                "4. **High-Value vs High-Count strategy?** \u2192 Both matter. Use segment-specific strategies as outlined in Section 8.1."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## \ud83d\udce4 9. Save Models for Deployment\n",
                "\n",
                "These pickled models are loaded by the **Streamlit** dashboard (`app.py`) for live predictions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save models for Streamlit deployment\n",
                "joblib.dump(kmeans, 'kmeans_model.pkl')\n",
                "joblib.dump(scaler, 'scaler.pkl')\n",
                "joblib.dump(lr, 'lr_model.pkl')\n",
                "\n",
                "print(\"\u2705 Models saved successfully!\")\n",
                "print(\"   \u2022 kmeans_model.pkl  \u2192 K-Means (4-cluster segmentation)\")\n",
                "print(\"   \u2022 scaler.pkl        \u2192 StandardScaler (feature normalization)\")\n",
                "print(\"   \u2022 lr_model.pkl      \u2192 LinearRegression (revenue forecasting)\")\n",
                "print(\"\\n\ud83d\ude80 These are used by the Streamlit app (app.py) for the interactive dashboard and predictor interface.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## \ud83d\ude80 Deployment\n",
                "\n",
                "The project is deployed using **Streamlit** and includes:\n",
                "\n",
                "1. **Interactive Dashboard** \u2014 Overview, Market Segments, Domains, Forecast pages\n",
                "2. **Prediction Interface** \u2014 Select Location + Domain + Transaction Volume to get a Revenue forecast\n",
                "3. **Clear Input-Output Demo** \u2014 Users can experiment with different combinations and see real-time results\n",
                "\n",
                "### To Run Locally:\n",
                "```bash\n",
                "pip install -r requirements.txt\n",
                "streamlit run app.py\n",
                "```\n",
                "\n",
                "### GitHub Repository\n",
                "The repository contains:\n",
                "- `bank_customer_segmentation.ipynb` \u2014 This notebook (analysis + models)\n",
                "- `app.py` \u2014 Streamlit application\n",
                "- `bankdataset.csv` \u2014 Dataset\n",
                "- `requirements.txt` \u2014 Dependencies\n",
                "- `README.md` \u2014 Detailed documentation"
            ]
        }
    ],
    "metadata": {
        "colab": {
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}